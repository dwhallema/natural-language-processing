{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text cleaning and natural language processing (Python, regex, NLTK, Gensim) \n",
    "\n",
    "Author: Dennis W. Hallema \n",
    "\n",
    "Description: Tools for extracting topics and other insights from raw text data, and presenting those insights. \n",
    "\n",
    "Depends: See `environment.yml`. \n",
    "\n",
    "Disclaimer: Use at your own risk. No responsibility is assumed for a user's application of these materials or related materials. \n",
    "\n",
    "Data: \n",
    "\n",
    "* Exerpt from *Nature* (10/2019), \"Quantum supremacy using a programmable superconducting processor.\" \n",
    "* Script of William Shakespeare's play *\"Troilus and Cressida\"* (c. 1602) \n",
    "* 12 Online Wikipedia articles on business performance \n",
    "\n",
    "<!-- \n",
    "Installation of polyglot:\n",
    "1. git clone https://github.com/aboSamoor/polyglot.git\n",
    "2. python setup.py install \n",
    "-->\n",
    "\n",
    "Contents:  \n",
    "* [Regular Expressions](#first-bullet)\n",
    "* [Tokenization with NLTK](#second-bullet)\n",
    "* [Non-ASCII tokenization (text with emojis)](#third-bullet)\n",
    "* [Charting sentence length](#fourth-bullet)\n",
    "* [Text cleaning and topic identification](#fifth-bullet)\n",
    "* [Creating and querying a corpus with gensim](#sixth-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of text parsing and text based search with regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fundamental challenge is to build a high-fidelity processor capable of running quantum algorithms in an exponentially large computational space. Here we report the use of a processor with programmable superconducting qubits to create quantum states on 53 qubits, corresponding to a computational state-space of dimension 2 to the power 53 (about 10 to the power 16).\n"
     ]
    }
   ],
   "source": [
    "# Import regex module\n",
    "import regex as re\n",
    "\n",
    "# Create str variable (source: https://www.nature.com/articles/s41586-019-1666-5)\n",
    "my_string = \"A fundamental challenge is to build a high-fidelity processor capable of running quantum algorithms in an exponentially large computational space. Here we report the use of a processor with programmable superconducting qubits to create quantum states on 53 qubits, corresponding to a computational state-space of dimension 2 to the power 53 (about 10 to the power 16).\"\n",
    "print(my_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A fundamental challenge is to build a high-fidelity processor capable of running quantum algorithms in an exponentially large computational space', ' Here we report the use of a processor with programmable superconducting qubits to create quantum states on 53 qubits, corresponding to a computational state-space of dimension 2 to the power 53 (about 10 to the power 16)', '']\n"
     ]
    }
   ],
   "source": [
    "# Split on sentence endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "print(re.split(sentence_endings, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here']\n",
      "['53', '2', '53', '10', '16']\n"
     ]
    }
   ],
   "source": [
    "# Find all capitalized words in my_string\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Find all digits in my_string\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'qubits' reported: ['53']\n"
     ]
    }
   ],
   "source": [
    "# Find how many \"qubits\" are reported in my_string (lookahead after match)\n",
    "qubits= r\"\\d+(?= qubits)\"\n",
    "print(\"Number of 'qubits' reported: \" + str(re.findall(qubits, my_string)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Regular Expressions:</i> In this text sample, Google describes a processor with 53 programmable superconducting quantum bits (or qubits). We could use the same 'lookahead after match' regex query if we wanted to generate an inventory of the number of qubits mentioned in other reports. We will not do that here, instead we'll explore the next topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization with NLTK <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK packages\n",
    "x = nltk.download('punkt', quiet = True)\n",
    "x = nltk.download('wordnet', quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENE II. Troy. A street.\n",
      "Enter Cressida and her man Alexander.\n",
      "\n",
      "CRESSIDA.\n",
      "Who were those went by?\n",
      "\n",
      "ALEXANDER.\n",
      "Queen Hecuba and Helen.\n",
      "\n",
      "CRESSIDA.\n",
      "And whither go they?\n",
      "\n",
      "ALEXANDER.\n",
      "Up to the eastern tow\n"
     ]
    }
   ],
   "source": [
    "# Import scene II of William Shakespeare's play \"Troilus and Cressida\"\n",
    "filename = \"data/troilus_scene2.txt\"\n",
    "with open(filename, 'r') as file:\n",
    "    scene_two = file.read()\n",
    "    \n",
    "# Print the beginning of the text\n",
    "print(scene_two[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who', 'were', 'those', 'went', 'by', '?']\n",
      "Number of unique tokens: 6\n"
     ]
    }
   ],
   "source": [
    "# Split scene_two into sentences\n",
    "sentences = sent_tokenize(scene_two)\n",
    "\n",
    "# Tokenize (break up into words) the sixth sentence\n",
    "sent_tokens = word_tokenize(sentences[5])\n",
    "print(sent_tokens)\n",
    "\n",
    "# Count the number of tokens\n",
    "print(\"Number of unique tokens: \" + str(len(sent_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here', 'wit', 'maxim', 'bak', 'thousand', 'though', 'about', 'cares', '-Good', 'think', 'Paris', 'mercy', 'Men', 'an', 'know', 'dolts', 'choose', 'like', 'Look', 'Upon', 'Phrygia', 'fire', 'It', 'wooing', 'whither', 'virtue', 'higher', 'forked', 'Troilus-', 'toward', 'been', 'Faith', 'men', 'sauced', 'i', 'who', \"'Jupiter\", \"E'en\", \"'d\", 'Greece', 'tapster', 'attaint', 'bear', 'cop', 'uncle', 'Things', 'patience', 'heart', 'Whose', 'Is', 'height', 'up', 'is', 'everything', 'did', 'much', 'lifter', 'Pardon', 'Swords', \"'Two\", 'boy', 'th', 'am', 'laugh', 'prize', \"'and\", 'prophet', 'pass', 'melancholy', \"to't\", 'humours', 'firm', 'say', 'thing', 'admirable', 'eyes', 'judgement', 'noise', 'meat', 'lie', 'II', 'stain', 'my', 'nature', 'may', 'field', 'had', 'chickens', 'crush', 'cloven', 'watch', 'prove', 'appear', 'soldiers', 'Deiphobus', 'crowded', 'whose', 'How', 'By', 'become']\n",
      "Number of tokens: 737\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the entire scene\n",
    "scene_tokens = set(word_tokenize(scene_two))\n",
    "\n",
    "# Print the tokens\n",
    "print(list(scene_tokens)[:100])\n",
    "\n",
    "# Count the number of tokens\n",
    "print(\"Number of tokens: \" + str(len(scene_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270 276\n"
     ]
    }
   ],
   "source": [
    "# Search for the first occurrence of \"Hector\" in scene_two and print \n",
    "match = re.search(\"Hector\", scene_two)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-ASCII tokenization (text with emojis) <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tu', 'viens', 'me', 'chercher', 'STP', 'üòÄüôè']\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "from nltk.tokenize import regexp_tokenize, TweetTokenizer\n",
    "\n",
    "# Create str variable\n",
    "french_text = \"Tu viens me chercher STP üòÄüôè\"\n",
    "\n",
    "# Tokenize all words in french_text\n",
    "all_words = word_tokenize(french_text)\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tu', 'STP']\n",
      "['üòÄ', 'üôè']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize only capital words\n",
    "capital_words = r\"[A-Z|√ú]\\w+\"\n",
    "print(regexp_tokenize(french_text, capital_words))\n",
    "\n",
    "# Tokenize only emojis\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(french_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Non-ASCII tokenization:</i> The above example demonstrates that tokenization is not limited to ASCII text. UTF character encodings (including emojis) can also be tokenized. This is useful in sentiment analysis, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charting sentence length <a class=\"anchor\" id=\"fourth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENE: Troy and the Greek camp before it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROLOGUE\n",
      "In Troy, there lies the scene. From isles of Greece\n",
      "The princes orgulous, their high blood chaf'd,\n",
      "Have to the port of Athens sent their ships\n",
      "Fraught with the ministers and instruments\n",
      "Of cruel war. Sixty and nine that wore\n",
      "Their crownets regal from the Athenian bay\n",
      "Put forth toward Phrygia; and their vow is made\n",
      "To ransack Troy, within whose\n"
     ]
    }
   ],
   "source": [
    "# Import the whole script of William Shakespeare's \"Troilus and Cressida\"\n",
    "filename = \"data/troilus.txt\"\n",
    "with open(filename, 'r') as file:\n",
    "    troilus = file.read()\n",
    "    \n",
    "# Print the beginning of the text\n",
    "print(troilus[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts still there: ['TROILUS.', 'Sweet Pandarus-', '', 'PANDARUS.', 'Pray you, speak no more to me: I will leave all as I found it, and there an end.', '', '[Exit Pandarus. An alarum.]']\n"
     ]
    }
   ],
   "source": [
    "# Split the script into lines\n",
    "lines = troilus.split('\\n')\n",
    "\n",
    "# Print lines\n",
    "print(\"Prompts still there: \" + str(lines[160:167]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts replaced with empty str: ['', 'Sweet Pandarus-', '', '', 'Pray you, speak no more to me: I will leave all as I found it, and there an end.', '', '']\n"
     ]
    }
   ],
   "source": [
    "# Remove prompts like 'TROILUS.', 'ACT 1' and '[Exit Pandarus.]' with regex\n",
    "pattern = \"^\\[(.*?)\\]$|^[A-Z]{2,}.*|^Enter.*$\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Print lines again\n",
    "print(\"Prompts replaced with empty str: \" + str(lines[160:167]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import modules\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Tokenize each line\n",
    "tokenized_lines = [regexp_tokenize(s, r\"\\w+\") for s in lines]\n",
    "\n",
    "# Make a frequency list of lengths\n",
    "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "# Plot a histogram of sentence lengths with collection bin for high values\n",
    "bins = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 200]\n",
    "plt.xlim([0, bins[-2] + 1])\n",
    "plt.hist(np.clip(line_num_words, bins[0], bins[-1]), bins=bins)\n",
    "plt.gca().set(title = \"Shakespeare\\'s play \\\"Troilus and Cressida\\\" (c. 1602)\", \n",
    "              xlabel = \"Sentence length (in words)\", ylabel = \"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Charting sentence length with NLTK:</i> The histogram shows that most lines are non-dialogue lines (speaker prompts, stage instructions or vertical space). Dialogue lines are typically 9 words long. There are also lines of 15 words and longer, meaning that we should insert line breaks to make it easier to read the script on stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning and topic identification <a class=\"anchor\" id=\"fifth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENE: Troy and the Greek camp before it\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PROLOGUE\n",
      "In Troy, there lies the scene. From isles of Greece\n",
      "The princes orgulous, their high blood chaf'd,\n",
      "Have to the port of Athens sent their ships\n",
      "Fraught with the ministers and instruments\n",
      "Of cruel war. Sixty and nine that wore\n",
      "Their crownets regal from the Athenian bay\n",
      "Put forth toward Phrygia; and their vow is made\n",
      "To ransack Troy, within whose\n"
     ]
    }
   ],
   "source": [
    "# Import William Shakespeare's play \"Troilus and Cressida\"\n",
    "filename = \"data/troilus.txt\"\n",
    "with open(filename, 'r') as file:\n",
    "    troilus = file.read()\n",
    "\n",
    "# Print the beginning of the text\n",
    "print(troilus[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 2881), (',', 2620), ('the', 816), ('and', 793), ('i', 604), (';', 584), ('to', 531), ('of', 507), ('a', 465), ('you', 441)]\n"
     ]
    }
   ],
   "source": [
    "# Building a Counter with bag-of-words\n",
    "\n",
    "# Import module\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(troilus)\n",
    "\n",
    "# Convert the tokens into lowercase\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common tokens (printed above) are punctuation and stop words but these are not useful in topic identification. We will therefore clean with regex and NLTK:\n",
    "<ol>\n",
    "    <li>Remove paragraphs we do not want to analyze</li>\n",
    "    <li>Remove numbers and punctiation</li>\n",
    "    <li>Remove stop words</li>\n",
    "    <li>Lemmatize the remaining words</li>\n",
    "</ol>\n",
    "Lemmatization groups inflected forms of a word so they can be analyzed as a single item. This is similar to stemming except it also performs morphological analysis, and links words with similar meaning to one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In Troy, there lies the scene. From isles of Greece\n",
      "The princes orgulous, their high blood chaf'd,\n",
      "Have to the port of Athens sent their ships\n",
      "Fraught with the ministers and instruments\n",
      "Of cruel war. Sixty and nine that wore\n",
      "Their crownets regal from the Athenian bay\n",
      "Put forth toward Phrygia;\n"
     ]
    }
   ],
   "source": [
    "# Text cleaning with regex\n",
    "\n",
    "# Split the text into lines\n",
    "lines = troilus.split(\"\\n\")\n",
    "\n",
    "# Remove paragraphs we do not want to analyze (prompts \n",
    "# like 'TROILUS.', 'ACT 1' and '[Exit Pandarus.]')\n",
    "pattern = \"^\\[(.*?)\\]$|^[A-Z]{2,}.*|^Enter.*$\"\n",
    "lines = [re.sub(pattern, '', l) for l in lines]\n",
    "\n",
    "# Recombine lines\n",
    "troilus_body = '\\n'.join(lines)\n",
    "print(troilus_body[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Text cleaning with regex:</i> The text (printed above) no longer contains prompts like 'TROILUS.', 'ACT 1' and '[Exit Pandarus.]'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common tokens: [('shall', 126), ('hector', 121), ('come', 120), ('lord', 114), ('go', 85), ('troilus', 84), ('let', 79), ('love', 79), ('man', 77), ('good', 76), ('achilles', 74), ('would', 70), ('know', 63), ('like', 61), ('great', 61), ('say', 60), ('troy', 59), ('well', 59), ('tell', 58), ('one', 56)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokens = word_tokenize(troilus_body)\n",
    "\n",
    "# Convert the tokens into lowercase\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Remove numbers and punctation\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove stop words\n",
    "english_stops = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just','don','should','now','d','ll','m','o','re','ve','y','ain','aren','couldn','didn','doesn','hadn','hasn','haven','isn','ma','mightn','mustn','needn','shan','shouldn','wasn','weren','won','']\n",
    "earlymodern_stops = ['art','doth','dost','\\'ere','hast','hath','hence','hither','nigh','oft','should\\'st','thither','tither','thee','thou','thine','thy','\\'tis','\\'twas','wast','whence','wherefore','whereto','withal','would\\'st','ye','yon','yonder']\n",
    "no_stops = [t for t in alpha_only if t not in (english_stops + earlymodern_stops)]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list (sort words by grouping inflected or variant forms of the same word)\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "print(\"Most common tokens: \" + str(bow.most_common(20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Topic identification of Shakespeare's \"Troilus and Cressida\":</i> The signature of Elizabethan stage plays is evident from the frequent occurrence of words like 'shall' and 'lord' (see above). Hector, Troilus and Achilles are the most frequently talked about characters in that particular order (remember that we removed the character prompts). Also frequently mentioned are 'love' and 'troy'. After cleaning, tokenizing, and lemmatizing the text with regex and NLTK, we now have a good idea of the who, what and where of the play without having read it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and querying a corpus with Gensim <a class=\"anchor\" id=\"sixth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A text corpus is a set of texts that we can analyze and query. Here, we'll download a series of articles from the web, build a corpus and perform analysis on the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import requests\n",
    "import bs4\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles retrieved: 12\n"
     ]
    }
   ],
   "source": [
    "# Create list of web article urls\n",
    "urls = [\"https://en.wikipedia.org/wiki/Revenue\",\n",
    "        \"https://en.wikipedia.org/wiki/Profit_margin\",\n",
    "        \"https://en.wikipedia.org/wiki/Gross_margin\",\n",
    "        \"https://en.wikipedia.org/wiki/Customer_acquisition_cost\",\n",
    "        \"https://en.wikipedia.org/wiki/Customer_retention\",\n",
    "        \"https://en.wikipedia.org/wiki/Loyalty_marketing\",\n",
    "        \"https://en.wikipedia.org/wiki/Net_Promoter\",\n",
    "        \"https://en.wikipedia.org/wiki/Lead_generation\",\n",
    "        \"https://en.wikipedia.org/wiki/Conversion_rate_optimization\",\n",
    "        \"https://en.wikipedia.org/wiki/Web_analytics\",\n",
    "        \"https://en.wikipedia.org/wiki/Benchmarking\",\n",
    "        \"https://en.wikipedia.org/wiki/Employee_engagement\"\n",
    "        ]\n",
    "\n",
    "# Create list variable for GET query results\n",
    "responses = []\n",
    "\n",
    "# Get web articles\n",
    "for url in urls:\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "    except HTTPError as http_err:\n",
    "        print(f'HTTP error occurred: {http_err}')\n",
    "    except Exception as err:\n",
    "        print(f'Exception occurred: {err}')\n",
    "    else:\n",
    "        \n",
    "        # Append query response\n",
    "        responses.append(r)\n",
    "        \n",
    "print(\"Number of articles retrieved: \" + str(len(responses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the GET query fails for an URL, the code above will skip the unresponsive URL and try next URL. The result is a list of length *n* containing the query results, where *n* equals the number of articles that were retrieved successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 0: In accounting, revenue is the income that a business has from its norm\n",
      "Article 1: Profit margin, net margin, net profit margin or net profit ratio is a \n",
      "Article 2: Gross margin is the difference between revenue and cost of goods sold \n",
      "Article 3: Customer Acquisition Cost (CAC) is the cost associated in convincing a\n",
      "Article 4: Customer retention refers to the ability of a company or product to re\n",
      "Article 5: Loyalty marketing is an approach to marketing, based on strategic mana\n",
      "Article 6: \n",
      "Net Promoter or Net Promoter Score (NPS) is a management tool that ca\n",
      "Article 7: \n",
      "\n",
      "In marketing, lead generation (/ÀàliÀêd/) is the initiation of consume\n",
      "Article 8: In internet marketing, and web analytics conversion optimization, or c\n",
      "Article 9: Web analytics is the measurement, collection, analysis and reporting o\n",
      "Article 10: Benchmarking is the practice of comparing business processes and perfo\n",
      "Article 11: Employee engagement is a fundamental concept in the effort to understa\n"
     ]
    }
   ],
   "source": [
    "# Create list variable for document tokens\n",
    "articles = []\n",
    "articles_text = []\n",
    "\n",
    "# Extract text, clean text, tokenize and lemmatize for each article\n",
    "for r in responses:\n",
    "    \n",
    "    # Extract text from HTML\n",
    "    html = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    paragraphs = html.select(\"p\")\n",
    "    text = '\\n'.join([ para.text for para in paragraphs])\n",
    "    articles_text.append(text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Convert the tokens into lowercase\n",
    "    lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "    # Remove numbers and punctation\n",
    "    alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "    # Remove stop words\n",
    "    english_stops = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','what','which','who','whom','this','that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most','other','some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just','don','should','now','d','ll','m','o','re','ve','y','ain','aren','couldn','didn','doesn','hadn','hasn','haven','isn','ma','mightn','mustn','needn','shan','shouldn','wasn','weren','won','']\n",
    "    no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "    # Lemmatize all tokens into a new list\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "    \n",
    "    # Append lemmatized to list of document tokens\n",
    "    articles.append(lemmatized)\n",
    "\n",
    "# Print the beginning of each article\n",
    "x = [print(\"Article \" + str(i) + \": \" + t[:70]) for i,t in enumerate(articles_text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 12 articles above are all related to business performance. For each of the articles we downloaded the HTML, extracted the text from HTML, cleaned the text, tokenized and lemmatized the tokens. Finally, we combined the result in a list variable. Next, we will create a corpus of these articles, and perform queries on the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost\n"
     ]
    }
   ],
   "source": [
    "# Create a Dictionary from the articles\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"cost\"\n",
    "cost_id = dictionary.token2id.get(\"cost\")\n",
    "\n",
    "# Use score_id with the dictionary to print the word\n",
    "print(dictionary.get(cost_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, 2), (29, 6), (30, 1), (44, 3), (57, 15), (93, 1), (94, 1), (101, 1), (107, 1), (121, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the third document\n",
    "print(corpus[3][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "margin 63\n",
      "gross 39\n",
      "cost 35\n",
      "profit 27\n",
      "sale 22\n"
     ]
    }
   ],
   "source": [
    "# Gensim bag-of-words\n",
    "\n",
    "# Import modules\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Save the third document\n",
    "doc = corpus[2]\n",
    "\n",
    "# Sort the doc for frequency\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the most frequently occurring words in the third document\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer 130\n",
      "company 89\n",
      "revenue 79\n",
      "margin 73\n",
      "lead 73\n",
      "cost 68\n",
      "marketing 63\n",
      "web 59\n",
      "business 58\n",
      "sale 55\n"
     ]
    }
   ],
   "source": [
    "# Create the defaultdict\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Print the most frequently occurring words in the corpus dictionary\n",
    "w2cSorted = dict(sorted(total_word_count.items(), key=lambda x: x[1],reverse=True)[:10])\n",
    "for word_id in w2cSorted:\n",
    "    print(dictionary.get(word_id), w2cSorted[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Text corpus of business performance articles: </i> Word counts on the whole corpus (above) suggest that 'customer', 'company' and 'revenue' are the most important concepts discussed in these 12 articles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
